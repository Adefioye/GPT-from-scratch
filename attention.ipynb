{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a94b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gpt_scratch/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3115a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f1258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    enc_text = tokenizer.encode(raw_text)\n",
    "    print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e7cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
    "                         shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c8785fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2279bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dcba5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=8, stride=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1998ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464, 1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899]])]\n",
      "[tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8307fd9",
   "metadata": {},
   "source": [
    "## Self attention\n",
    "\n",
    "The goal of self attention is to compute the `context vector` of each token in a sequence. The context vector is an enriched embedding representation of a token. It is packed with information about the token itself and its relationship/relevance to other tokens in a sequence.\n",
    "\n",
    "#### Implement self-attention with untrainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3288663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf94e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector:\n",
      " tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "# Normalize the attention scores with a softmax\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "# Compute now the context vector\n",
    "context_vector = attention_weights @ inputs\n",
    "print(\"Context vector:\\n\", context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9ac87",
   "metadata": {},
   "source": [
    "#### Implement self-attention with trainable weights\n",
    "\n",
    "In self-attention without `trainaible weights`, `context vector` is a weighted sum over input vectors. However, for `trainable weights`, `context vector` is weighted sum over value vector.\n",
    "\n",
    "##### Query, Key and Value analogy to database operation\n",
    "`query` is the word/token in input sequence that the model wants to get information on, key is what is used to get the information about the query and `value` is the information received using the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786872ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Let's now implement self-attention with trainable weights\n",
    "# We will be using nn.Parameter to initialize and create the weights\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        \"\"\"\n",
    "        :param d_in: input dimension\n",
    "        :param d_out: output dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_k = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_v = nn.Parameter(torch.randn(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input tensor of shape (batch_size, d_in)\n",
    "        :return: output tensor of shape (batch_size, d_out)\n",
    "        \"\"\"\n",
    "        # Compute queries, keys and values\n",
    "        Q = x @ self.W_q\n",
    "        K = x @ self.W_k\n",
    "        V = x @ self.W_v\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = Q @ K.T\n",
    "        attention_weights = torch.softmax(attention_scores / (K.shape[-1] ** 0.5), dim=-1)\n",
    "\n",
    "        # Compute context vector\n",
    "        context_vector = attention_weights @ V\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff822e02",
   "metadata": {},
   "source": [
    "- Let's test self-attention v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02ba42bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2845, 0.4071],\n",
       "        [0.2854, 0.4081],\n",
       "        [0.2854, 0.4075],\n",
       "        [0.2864, 0.3974],\n",
       "        [0.2863, 0.3910],\n",
       "        [0.2860, 0.4039]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "d_in, d_out = 3, 2\n",
    "sa_v1 = SelfAttention_v1(d_in=d_in, d_out=d_out)\n",
    "context_vector_v1 = sa_v1(inputs) # Context vector for the inputs\n",
    "context_vector_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "324c0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now use nn.Linear to implement self-attention\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bais=False):\n",
    "        \"\"\"\n",
    "        :param d_in: input dimension\n",
    "        :param d_out: output dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bais)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bais)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bais)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input tensor of shape (batch_size, d_in)\n",
    "        :return: output tensor of shape (batch_size, d_out)\n",
    "        \"\"\"\n",
    "        # Compute queries, keys and values\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = Q @ K.T\n",
    "        attention_weights = torch.softmax(attention_scores / (K.shape[-1] ** 0.5), dim=-1)\n",
    "\n",
    "        # Compute context vector\n",
    "        context_vector = attention_weights @ V\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4aa3d4",
   "metadata": {},
   "source": [
    "- Let's now test self-attention v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75db67fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out)\n",
    "context_vector_v2 = sa_v2(inputs) # Context vector for the inputs\n",
    "context_vector_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03598e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Let's test if the two context vectors are the same\n",
    "print(torch.allclose(context_vector_v1, context_vector_v2, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d600f89",
   "metadata": {},
   "source": [
    "- Let's find a way to clone the weights from v2 and copy to v1 and then check if they are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0c0d74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Lets clone the weights of self-attention v2 to self-attention v1\n",
    "torch.manual_seed(123)\n",
    "sa_v1.W_q.data = sa_v2.W_q.weight.data.T.clone()\n",
    "sa_v1.W_k.data = sa_v2.W_k.weight.data.T.clone()\n",
    "sa_v1.W_v.data = sa_v2.W_v.weight.data.T.clone()\n",
    "\n",
    "context_vector_v1 = sa_v1(inputs) \n",
    "context_vector_v2 = sa_v2(inputs)\n",
    "\n",
    "# Let's test if the two context vectors are the same\n",
    "print(torch.allclose(context_vector_v1, context_vector_v2, atol=1e-6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f827b389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e56e8629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78cd6c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
       "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
       "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
       "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
       "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's experiment with implementing causal self-attention using self-attention v2\n",
    "queries = sa_v2.W_q(inputs)\n",
    "keys = sa_v2.W_k(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / (d_out ** 0.5), dim=-1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63afb0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = attention_scores.shape[0]\n",
    "causal_mask = torch.tril(torch.ones(context_length, context_length))\n",
    "causal_mask * attention_weights # This helps to mask the future tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d5ba4",
   "metadata": {},
   "source": [
    "#### Implementing a compact causal attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6b30a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets implement causal self attention with dropout\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, qkv_bias=False, dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param d_in: input dimension\n",
    "        :param context_length: length of the context\n",
    "        :param qkv_bais: whether to use bias in the linear layers\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Register a buffer for the causal mask\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(context_length, context_length)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input tensor of shape (B, T, d_in)\n",
    "        :return: output tensor of shape (B, T, d_out)\n",
    "        \"\"\"\n",
    "        # Expect input shape to be (B, T, d_in)\n",
    "        B, T, d_in = x.shape\n",
    "        # Compute queries, keys and values\n",
    "        Q = self.W_q(x) # (B, T, d_out)\n",
    "        K = self.W_k(x) # (B, T, d_out)\n",
    "        V = self.W_v(x) # (B, T, d_out)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = Q @ K.transpose(-1, -2) # (B, T, d_out) @ (B, d_out, T) -> (B, T, T)\n",
    "\n",
    "        # Apply causal mask\n",
    "        attention_scores = attention_scores.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        attention_weights = torch.softmax(attention_scores / (K.shape[-1] ** 0.5), dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        # Compute context vector\n",
    "        context_vector = attention_weights @ V # (B, T, T) @ (B, T, d_out) -> (B, T, d_out)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c79a962d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack([inputs, inputs], dim=0)\n",
    "context_length = batch.shape[1]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63ad5424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06e9d905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "causal_attention = CausalSelfAttention(d_in=d_in, d_out=d_out, context_length=context_length)\n",
    "context_vector = causal_attention(batch)\n",
    "context_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b90945e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5526, -0.0981],\n",
       "         [-0.5299, -0.1081]],\n",
       "\n",
       "        [[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5526, -0.0981],\n",
       "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6b3ff",
   "metadata": {},
   "source": [
    "#### Implement vanilla MultiHeadAttention Wrapper with sequential head processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d0c0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, n_heads, qkv_bias=False, dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param d_in: input dimension\n",
    "        :param n_heads: number of heads\n",
    "        :param context_length: length of the context\n",
    "        :param qkv_bias: whether to use bias in the linear layers\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads= nn.ModuleList([\n",
    "            CausalSelfAttention(d_in=d_in, d_out=d_out, context_length=context_length, qkv_bias=qkv_bias, dropout=dropout)\n",
    "            for _ in range(n_heads)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input tensor of shape (B, T, d_in)\n",
    "        :return: output tensor of shape (B, T, d_out * n_heads)\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22c62f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set d_out to 1 so output dimension of context vector is 2 since we have 2 heads\n",
    "mha = MultiHeadAttentionWrapper(d_in=d_in, d_out=1, context_length=context_length, n_heads=2)\n",
    "context_vector = mha(batch)\n",
    "context_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b2f111",
   "metadata": {},
   "source": [
    "#### Implement MultiHeadAttention with parallel head processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a87a980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's implement multi-head attention with parallel head processing\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, n_heads, qkv_bias=False, dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param d_in: input dimension\n",
    "        :param n_heads: number of heads\n",
    "        :param context_length: length of the context\n",
    "        :param qkv_bias: whether to use bias in the linear layers\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_out % n_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "        self.head_dim = d_out // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_projection = nn.Linear(d_out, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Register a buffer for the causal mask\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input tensor of shape (B, T, d_in)\n",
    "        :return: output tensor of shape (B, T, d_out)\n",
    "        \"\"\"\n",
    "        # Expect input shape to be (B, T, d_in)\n",
    "        B, T, d_in = x.shape\n",
    "        # Compute queries, keys and values\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        # Reshape Q, K, V to (B, n_heads, T, head_dim)\n",
    "        Q = Q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hdim)\n",
    "        K = K.view(B, T, self.n_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hdim)\n",
    "        V = V.view(B, T, self.n_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hdim)  \n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = Q @ K.transpose(-1, -2) # (B, nh, T, hdim) @ (B, nh, hdim, T) -> (B, nh, T, T)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        attention_scores = attention_scores.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        attention_weights = torch.softmax(attention_scores / (K.shape[1] ** 0.5), dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        # Compute context vector\n",
    "        context_vector = attention_weights @ V # (B, nh, T, T) @ (B, nh, T, hdim) -> (B, nh, T, hdim)\n",
    "        # Reshape context vector to (B, T, d_out)\n",
    "        context_vector = context_vector.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        # Apply output projection\n",
    "        context_vector = self.out_projection(context_vector) # (B, T, d_out)\n",
    "        return context_vector\n",
    "torch.manual_seed(123)\n",
    "d_in, d_out = 3, 2\n",
    "context_length = 6\n",
    "mha_v2 = MultiHeadAttention(d_in=d_in, d_out=d_out, context_length=context_length, n_heads=2)\n",
    "context_vector = mha_v2(batch)\n",
    "context_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adf18d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1257, -0.1968],\n",
       "         [ 0.1007, -0.2920],\n",
       "         [ 0.0920, -0.3225],\n",
       "         [ 0.0758, -0.2946],\n",
       "         [ 0.0702, -0.2891],\n",
       "         [ 0.0641, -0.2792]],\n",
       "\n",
       "        [[ 0.1257, -0.1968],\n",
       "         [ 0.1007, -0.2920],\n",
       "         [ 0.0920, -0.3225],\n",
       "         [ 0.0758, -0.2946],\n",
       "         [ 0.0702, -0.2891],\n",
       "         [ 0.0641, -0.2792]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3f566dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2359296"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets now check number of parameters in the multi-head attention layer in GPT-2\n",
    "context_length = 1024\n",
    "d_in, d_out = 768, 768\n",
    "num_heads = 12\n",
    "\n",
    "mha = MultiHeadAttention(d_in=d_in, d_out=d_out, context_length=context_length, n_heads=num_heads)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(mha) # (2.36 M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc1e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
